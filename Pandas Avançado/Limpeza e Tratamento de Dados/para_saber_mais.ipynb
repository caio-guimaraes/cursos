{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para saber mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - O que são modelos de machine learning?\n",
    "\n",
    "Modelos de machine learning são um tipo de tecnologia que permite que computadores aprendam a realizar tarefas específicas sem serem explicitamente programados para isso. Esses modelos são criados por meio de algoritmos que analisam dados e identificam padrões para criar uma representação matemática da relação entre os dados de entrada e de saída.\n",
    "\n",
    "Para entender melhor, imagine que você quer criar um modelo para prever se uma pessoa vai gostar ou não de um filme baseado em características, como: gênero, elenco, sinopse, diretor(a), etc. Você começa alimentando o modelo com um conjunto de dados que contém informações sobre os filmes e as avaliações das pessoas. O modelo usa esses dados para encontrar padrões e criar uma equação matemática que pode ser usada para prever se uma pessoa vai gostar ou não de um determinado filme.\n",
    "\n",
    "Outro exemplo de modelo de machine learning é a recomendação de produtos em sites de compras online como a Amazon. O modelo recebe dados de compra e histórico de navegação da pessoa usuária e usa esses dados para recomendar produtos que a pessoa pode gostar.\n",
    "\n",
    "Os modelos de Machine Learning são amplamente utilizados em diversas áreas, como: detecção de fraudes em transações financeiras, reconhecimento de voz e imagem, diagnóstico médico, entre outros. Eles são capazes de realizar tarefas complexas de forma muito mais rápida e eficiente e têm a capacidade de aprender e melhorar continuamente com o tempo, tornando-se cada vez mais precisos e confiáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - O que é Churn?\n",
    "\n",
    "Churn é um termo utilizado para se referir à taxa de cancelamento de clientes em um determinado período de tempo. Em outras palavras, é a porcentagem de clientes que deixam de usar um produto ou serviço em um determinado período.\n",
    "\n",
    "O Churn é importante por ser um indicador da saúde financeira e do desempenho de uma empresa. Uma taxa alta de Churn pode significar que a empresa está perdendo clientes mais rapidamente do que está adquirindo novos, o que pode ser um sinal de problemas com o produto ou serviço oferecido, com a experiência do(a) cliente ou com a concorrência.\n",
    "\n",
    "Por outro lado, uma taxa baixa de Churn pode indicar que a empresa está retendo clientes e oferecendo um bom serviço ou produto. É importante monitorar a taxa de Churn e trabalhar para mantê-la o mais baixa possível, a fim de garantir a satisfação e fidelidade dos(as) clientes.\n",
    "\n",
    "A importância de desenvolver um modelo de machine learning capaz de prever o Churn é que isso pode ajudar as empresas a identificar clientes que estão em risco de cancelar o serviço ou produto. Através de um modelo de machine learning, a empresa pode analisar dados históricos de clientes, compreendendo melhor as causas do Churn e, com base nessas informações, tomar medidas proativas para reter essas pessoas, como: oferecer descontos, melhorar o serviço ou enviar ofertas personalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros do ```json_normalize()```\n",
    "\n",
    "O método ```json_normalize()``` é usado para normalizar dados JSON em um formato tabular (como um DataFrame do Pandas) para facilitar a análise e manipulação. Os parâmetros mais importantes do método ```json_normalize()``` são:\n",
    "\n",
    "- ```data```: o objeto JSON a ser normalizado.\n",
    "- ```record_path```: um caminho para acessar o array de registros dentro do objeto JSON.\n",
    "- ```meta```: uma lista de colunas adicionais a serem incluídas no DataFrame, além das colunas normalizadas.\n",
    "- ```errors```: como lidar com erros de normalização. Os valores possíveis são \"raise\" (lançar um erro), \"ignore\" (ignorar o erro).\n",
    "- ```sep```: separador de colunas usado para concatenar as chaves do objeto JSON aninhado. O padrão é \".\".\n",
    "Aqui está um exemplo de como usar o método ```json_normalize()``` com alguns dos parâmetros mencionados acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>endereço.cidade</th>\n",
       "      <th>endereço.estado</th>\n",
       "      <th>empresa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>SP</td>\n",
       "      <td>alura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>RJ</td>\n",
       "      <td>alura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nome endereço.cidade endereço.estado empresa\n",
       "0  Alice       São Paulo              SP   alura\n",
       "1    Bob  Rio de Janeiro              RJ   alura"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"empresa\": \"alura\",\n",
    "    \"funcionarios\": [\n",
    "        {\"nome\": \"Alice\", \"endereço\": {\"cidade\": \"São Paulo\", \"estado\": \"SP\"}},\n",
    "        {\"nome\": \"Bob\", \"endereço\": {\"cidade\": \"Rio de Janeiro\", \"estado\": \"RJ\"}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.json_normalize(data, record_path='funcionarios', meta = \"empresa\", errors='ignore')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código em Python faz uso das bibliotecas pandas para criar um DataFrame que contém informações sobre pessoas associadas a uma empresa.\n",
    "\n",
    "O objeto data contém uma chave ```\"empresa\"``` com o valor ```\"alura\"```, bem como uma chave ```\"funcionarios\"``` cujo valor é uma lista contendo dois dicionários. Cada dicionário na lista representa uma pessoa, com informações sobre o nome e o endereço. O endereço é um dicionário aninhado com as chaves ```\"cidade\"``` e ```\"estado\"```.\n",
    "\n",
    "A função pd.```json_normalize()``` é usada para normalizar os dados em uma tabela. Nesse caso, ```data``` é passado como o objeto JSON a ser normalizado. O parâmetro ```record_path``` é definido como ```'funcionarios'``` para indicar que os dados das pessoas devem ser normalizados. O parâmetro ```meta``` é definido como ```\"empresa\"``` para incluir a informação da empresa em cada linha da tabela. Finalmente, o parâmetro ```errors``` é definido como ```'ignore'``` para ignorar quaisquer erros que possam ocorrer durante a normalização.\n",
    "\n",
    "A saída do código é um objeto DataFrame que contém informações sobre as pessoas associadas à empresa, com cada pessoa representada em uma linha e as informações de nome, cidade e estado em colunas separadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Por que alterar o tipo da coluna?\n",
    "\n",
    "Fazer o *cast* das colunas para as tipagens corretas é importante antes de inserir em um modelo de machine learning por diversos motivos:\n",
    "\n",
    "1. **Precisão dos resultados** Ao fazer o cast das colunas para suas tipagens corretas, garantimos que os dados estejam representados de forma precisa e sem perda de informações. Por exemplo, se uma coluna contém números decimais e for representada como um tipo de dado inteiro, a precisão será perdida, o que pode levar a resultados incorretos no modelo de machine learning.\n",
    "\n",
    "2. **Velocidade de processamento** Quando os dados são armazenados em seu tipo de dado correto, o processamento dos dados é mais rápido. Por exemplo, o uso de floats de menor precisão como float32 em vez de float64 pode reduzir a carga computacional em operações de ponto flutuante já que exigem menos cálculos e até mesmo reduz o armazenamento.\n",
    "\n",
    "3. **Compatibilidade do modelo** Modelos de machine learning podem ter requisitos específicos em relação aos tipos de dados que podem ser usados como entrada. Portanto, fazer o cast das colunas para as tipagens corretas garante a compatibilidade do modelo.\n",
    "\n",
    "4. **Evita erros** Quando as colunas são convertidas para seus tipos corretos, erros de dados são menos propensos a ocorrer, porque, em machine learning, o modelo é treinado com base em padrões e relações identificadas nos dados de treinamento. Se os dados contêm erros, isso pode afetar a precisão do modelo e fazer com que ele produza resultados imprecisos.\n",
    "\n",
    "Em resumo, fazer o cast das colunas para suas tipagens corretas é uma etapa importante na preparação dos dados para serem processados por um modelo de machine learning e pode ajudar a garantir que o modelo seja preciso e eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Tratando dados nulos\n",
    "\n",
    "Quando trabalhamos com dados em uma análise ou em um modelo de machine learning, é importante garantir que esses dados sejam precisos e completos. **Dados nulos**, também conhecidos como dados faltantes ou ausentes, são um problema comum que pode prejudicar a precisão de qualquer análise ou modelo de machine learning.\n",
    "\n",
    "Os dados nulos ocorrem quando não há informações disponíveis para determinada observação em um conjunto de dados. Isso pode acontecer por várias razões, como falhas no registro dos dados, perda de informações ou até mesmo erros humanos. Quando esses dados nulos são inseridos em um modelo de machine learning, o modelo pode interpretar erroneamente a falta de informação como algo significativo, resultando em um modelo enviesado ou impreciso, ou muitas vezes o algoritmo nem ao menos roda.\n",
    "\n",
    "Por isso, é importante tratar esses dados antes de inserir em um modelo de machine learning. Existem várias técnicas que podem ser utilizadas para tratar dados nulos, como: a exclusão das observações que possuem dados faltantes e a imputação de valores faltantes com base em outros dados do próprio conjunto de informações.\n",
    "\n",
    "A exclusão de observações com dados faltantes pode ser uma opção viável quando sua quantidade é pequena em comparação ao tamanho do conjunto de dados, ou quando a exclusão não prejudica a representatividade dos dados. No entanto, essa abordagem pode levar a perda de informações importantes e diminuir a capacidade do modelo de aprender.\n",
    "\n",
    "A imputação de valores faltantes é uma técnica comum para tratar dados nulos. Ela consiste em substituir os dados faltantes por um valor estimado com base em outros dados disponíveis. Existem vários métodos de imputação, como: média, mediana, moda ou até mesmo modelos de machine learning específicos para imputação.\n",
    "\n",
    "Existem alguns métodos da biblioteca Pandas que nos auxiliam a identificar quais são dados nulos em nosso DataFrame, como:\n",
    "\n",
    "- ```isnull()```: retorna um DataFrame de valores booleanos, no qual ```True``` indica o valor nulo e ```False``` indica o valor válido.\n",
    "- ```notnull()```: retorna um DataFrame de valores booleanos, no qual ```True``` indica o valor válido e ```False``` indica o valor nulo.\n",
    "- ```isna()```: retorna um DataFrame de valores booleanos, com ```True``` indicando o valor nulo e ```False``` o valor válido (a mesma funcionalidade do método ```isnull()```).\n",
    "- ```notna()```: retorna um DataFrame de valores booleanos, em que ```True``` indica o valor válido e ```False``` indica o valor nulo (a mesma funcionalidade do método ```notnull()```).\n",
    "\n",
    "Em resumo, o tratamento de dados nulos é fundamental para garantir que um modelo de machine learning tenha uma precisão elevada e uma capacidade de generalização adequada. O uso de técnicas adequadas de tratamento de dados nulos é um passo importante no processo de preparação de dados para qualquer análise ou modelo de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Inserindo a moda nos dados\n",
    "A imputação de valores nulos em um Pandas DataFrame é uma das técnicas úteis quando se trabalha com dados faltantes. A imputação é o processo de substituir os valores faltantes por outros valores, de modo a tornar o conjunto de dados mais completo e utilizável.\n",
    "\n",
    "A moda é uma medida estatística que representa o valor mais frequente em um conjunto de dados. Utilizá-la para preencher valores nulos é uma técnica simples e eficaz que pode ajudar a melhorar a qualidade dos dados.\n",
    "\n",
    "Imagine que você esteja trabalhando com um conjunto de dados que contém informações sobre vendas em uma loja. Algumas dessas informações podem estar faltando, como o tamanho de alguns produtos. Nesse caso, você pode usar a moda para preencher os valores nulos. Isso significa que você irá substituir os valores faltantes pelo tamanho mais comum dos produtos.\n",
    "\n",
    "Para fazer isso no Pandas, você pode seguir conforme código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Produto Tamanho   Preço\n",
      "0  Camisa       P   49.99\n",
      "1   Calça       M   79.99\n",
      "2   Tênis       M  199.99\n",
      "3    Meia       M    9.99\n",
      "4    Boné       M   39.99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "df = pd.DataFrame({'Produto': ['Camisa', 'Calça', 'Tênis', 'Meia', 'Boné'],\n",
    "                   'Tamanho': ['P', 'M', 'M', None, None],\n",
    "                   'Preço': [49.99, 79.99, 199.99, 9.99, 39.99]})\n",
    "\n",
    "# Preenchendo os valores nulos com a moda\n",
    "df['Tamanho'].fillna(df['Tamanho'].mode()[0], inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo, o DataFrame possui valores nulos na coluna \"Tamanho\". Para preencher esses valores com a moda, utilizamos o método \"fillna()\" em conjunto com o método \"mode()\". Esta retorna o valor mais comum em um conjunto de dados.\n",
    "\n",
    "No caso do DataFrame acima, a moda da coluna \"Tamanho\" é \"M\". Portanto, utilizamos esse valor para preencher os valores nulos na respectiva coluna. O resultado final é um DataFrame mais completo e utilizável, pronto para ser analisado e usado em futuras análises e decisões de negócios.\n",
    "\n",
    "Observe que os valores nulos foram substituídos pela moda.\n",
    "\n",
    "Para conferir com mais detalhes os métodos utilizados, você pode consultar suas documentações oficiais:\n",
    "- [método mode](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html)\n",
    "- [método fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Formas de identificar outliers\n",
    "\n",
    "Existem várias técnicas para detectar outliers em um conjunto de dados. Algumas das mais comuns são: Z-score, regra dos 3 sigmas e a análise de dispersão.\n",
    "\n",
    "1) **Z-score:**\n",
    "\n",
    "O z-score é uma medida estatística que indica a quantos múltiplos do desvio-padrão um dado está distante da média. Para detectar outliers utilizando essa medida, basta calcular o z-score de cada dado e verificar se ele está muito distante da média. Se o z-score for maior do que um determinado limite, podemos considerar esse dado como um outlier.\n",
    "\n",
    "O limite geralmente utilizado é de 3 desvios-padrões, ou seja, se o z-score de um dado for maior do que 3, podemos considerá-lo como um outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dados de exemplo\n",
    "data = np.array([10, 20, 30, 40, 150, 50, 60, 70, 80, 90, 100, 350])\n",
    "\n",
    "# Cálculo do z-score\n",
    "z_scores = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# Limite para considerar um dado como outlier\n",
    "limite = 3\n",
    "\n",
    "# Identificação dos outliers\n",
    "outliers = data[np.abs(z_scores) > limite]\n",
    "\n",
    "print(\"Outliers encontrados:\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Regra dos 3 sigmas:**\n",
    "\n",
    "A regra dos 3 sigmas é uma técnica que utiliza a distribuição normal para identificar outliers. Ela diz que cerca de 99,7% dos dados estarão dentro de 3 desvios padrão da média. Portanto, se um dado estiver a mais de 3 desvios padrão da média, podemos considerá-lo como um outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Criar um array com os dados\n",
    "dados = np.array([0, 10, 12, 13, 15, 16, 18, 20, 22, 25, 30, 35, 40, 50, 350])\n",
    "\n",
    "# Calcular a média e o desvio padrão do conjunto de dados\n",
    "media = np.mean(dados)\n",
    "desvio_padrao = np.std(dados)\n",
    "\n",
    "# Definir o limite superior e inferior para identificar os outliers\n",
    "limite_superior = media + (3 * desvio_padrao)\n",
    "limite_inferior = media - (3 * desvio_padrao)\n",
    "\n",
    "# Identificar os outliers no conjunto de dados\n",
    "outliers = dados[(dados > limite_superior) | (dados < limite_inferior)]\n",
    "\n",
    "print(\"Outliers:\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Análise de dispersão**\n",
    "\n",
    "A análise de dispersão é uma técnica que utiliza gráficos para identificar outliers. Um gráfico comumente utilizado é o boxplot que mostra a distribuição dos dados em quartis. Os outliers são identificados como pontos fora dos limites superior e inferior do boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dados de exemplo\n",
    "data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "\n",
    "# Criação do boxplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(data)\n",
    "\n",
    "# Identificação dos outliers\n",
    "outliers = data[(data < np.percentile(data, 25) - (1.5 * (np.percentile(data, 75) - np.percentile(data, 25)))) |\n",
    "                (data > np.percentile(data, 75) + (1.5 * (np.percentile(data, 75) - np.percentile(data, 25))))]\n",
    "\n",
    "print(\"Outliers encontrados:\", outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - O que são variáveis categóricas?\n",
    "As variáveis categóricas são um tipo de variável usado em Ciência de Dados para representar dados que podem ser classificados em diferentes categorias ou grupos. Por exemplo, se uma pessoa cientista de dados estiver analisando o desempenho acadêmico de estudantes em uma escola, ela pode usar variáveis categóricas para classificar o desempenho dessas pessoas em diferentes grupos, como \"excelente\", \"bom\" ou \"regular\". Isso permite identificar padrões ou tendências no desempenho.\n",
    "\n",
    "Existem muitos exemplos de variáveis categóricas na Ciência de Dados. Algumas das mais comuns incluem a cor dos olhos, o tipo sanguíneo, a marca de um carro ou a escolaridade. Essas variáveis não possuem uma escala numérica e não podem ser medidas em termos de magnitude ou intensidade. Em vez disso, elas são usadas para classificar dados em diferentes grupos ou categorias.\n",
    "\n",
    "Dentre os tipos de variáveis, temos:\n",
    "\n",
    "- **Variáveis categóricas nominais**\n",
    "São aquelas que não possuem uma ordem ou hierarquia específica entre as categorias. Por exemplo, se uma pessoa cientista de dados estiver analisando a preferência musical de um grupo de pessoas, ela pode usar variáveis categóricas nominais para classificar as pessoas em diferentes grupos, como: \"rock\", \"jazz\" ou \"pop\".\n",
    "\n",
    "- **Variáveis categóricas ordinais**\n",
    "Possuem uma ordem específica entre as categorias. Por exemplo, se a pessoa cientista de dados estiver analisando a escolaridade de um grupo de pessoas, ela pode usar esse tipo de variável para classificar as pessoas em diferentes grupos, como: \"ensino fundamental completo\", \"ensino médio completo\" ou \"ensino superior completo\" e assim por diante.\n",
    "\n",
    "- **Variáveis categóricas binárias**\n",
    "São um tipo especial de variável categórica que possui apenas duas categorias possíveis, por exemplo: sim/não, verdadeiro/falso ou presente/ausente. As variáveis categóricas binárias são úteis porque permitem que cientistas de dados analisem a distribuição de dados em apenas duas categorias possíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Parâmetros do get_dummies()\n",
    "O [método `get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) da biblioteca Pandas é utilizado para transformar variáveis categóricas em variáveis binárias. Abaixo estão os parâmetros do método:\n",
    "\n",
    "`data`: **parâmetro obrigatório** que representa o conjunto de dados e contém as variáveis categóricas a serem transformadas em variáveis binárias.\n",
    "\n",
    "`prefix`: é um parâmetro opcional utilizado para adicionar um prefixo às colunas binárias geradas pelo método `get_dummies()`. Por exemplo, se você definir o prefixo como \"cat_\", as colunas binárias geradas terão nomes como \"cat_1\", \"cat_2\", etc.\n",
    "\n",
    "`prefix_sep`: parâmetro opcional utilizado para definir o separador entre o prefixo e o nome original da coluna categórica. O valor padrão é \"_\".\n",
    "\n",
    "`columns`: parâmetro opcional utilizado para selecionar as colunas específicas do conjunto de dados que devem ser transformadas em variáveis binárias. Se não for especificado, todas as colunas categóricas serão transformadas.\n",
    "\n",
    "`drop_first`: é um parâmetro opcional utilizado para remover a primeira coluna binária gerada pelo método `get_dummies()`. Isso é feito para evitar a multicolinearidade, que é uma situação em que duas ou mais variáveis independentes estão altamente correlacionadas entre si.\n",
    "\n",
    "`dtype`: parâmetro opcional utilizado para definir o tipo de dado das colunas binárias geradas pelo método `get_dummies()`. O valor padrão é \"uint8\".\n",
    "\n",
    "Abaixo está um exemplo de como podemos aplicar esses parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formato</th>\n",
       "      <th>cor-verde</th>\n",
       "      <th>cor-vermelho</th>\n",
       "      <th>tam-médio</th>\n",
       "      <th>tam-pequeno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quadrado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>redondo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>redondo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quadrado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    formato  cor-verde  cor-vermelho  tam-médio  tam-pequeno\n",
       "0  quadrado          0             1          0            1\n",
       "1   redondo          0             0          1            0\n",
       "2   redondo          1             0          0            0\n",
       "3  quadrado          0             1          1            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "df = pd.DataFrame({'cor': ['vermelho', 'azul', 'verde', 'vermelho'],\n",
    "                   'tamanho': ['pequeno', 'médio', 'grande', 'médio'],\n",
    "                   'formato': ['quadrado', 'redondo', 'redondo', 'quadrado']})\n",
    "\n",
    "# Transformando as colunas categóricas em variáveis numéricas binárias\n",
    "df_dummies = pd.get_dummies(df, columns=['cor', 'tamanho'], prefix=['cor', 'tam'], prefix_sep='-', drop_first=True)\n",
    "\n",
    "# Exibindo o DataFrame resultante\n",
    "df_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, o parâmetro `data` é o DataFrame `df` que contém as colunas categóricas a serem transformadas. O parâmetro `columns` é uma lista que contém os nomes das colunas categóricas a serem transformadas em variáveis binárias. O parâmetro `prefix` é uma lista que contém os prefixos a serem adicionados às colunas binárias geradas. Aqui, estamos adicionando o prefixo \"cor_\" às colunas binárias geradas a partir da coluna \"cor\" e o prefixo \"tam_\" às colunas binárias geradas a partir da coluna \"tamanho\".\n",
    "\n",
    "Adicionamos também os parâmetros `prefix_sep='-'` para definir o separador entre o prefixo e o nome original da coluna categórica como um hífen (-). Também adicionamos o parâmetro drop_first=True para remover a primeira coluna binária gerada pelo método `get_dummies()`. Isso é feito para evitar a multicolinearidade, que é uma situação em que duas ou mais variáveis independentes estão altamente correlacionadas entre si.\n",
    "\n",
    "O resultado será um novo DataFrame chamado `df_dummies`, que contém as colunas originais do DataFrame `df`, bem como as novas colunas binárias geradas pelo método `get_dummies()` com um hífen (-) como separador de prefixo e a primeira coluna binária removida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
